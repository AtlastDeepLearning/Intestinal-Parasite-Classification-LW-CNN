{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AtlastDeepLearning/Intestinal-Parasite-Classification-LW-CNN/blob/main/Intestinal_Parasite_LW_CNN_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4pnkUWcoT5D8",
        "outputId": "0150cd9b-77b8-4384-8d85-0d0d543caf95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://google-coral.github.io/py-repo/\n",
            "Collecting tflite-runtime\n",
            "  Downloading tflite_runtime-2.14.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from tflite-runtime) (2.0.2)\n",
            "Downloading tflite_runtime-2.14.0-cp311-cp311-manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tflite-runtime\n",
            "Successfully installed tflite-runtime-2.14.0\n",
            "Collecting efficientnet\n",
            "  Downloading efficientnet-1.1.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from efficientnet) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.14.0)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet) (1.15.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet) (2025.6.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet) (0.4)\n",
            "Downloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-applications, efficientnet\n",
            "Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n",
            "Collecting imgaug\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from imgaug) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.11/dist-packages (from imgaug) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imgaug) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from imgaug) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from imgaug) (3.10.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.11/dist-packages (from imgaug) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from imgaug) (4.11.0.86)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from imgaug) (2.37.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from imgaug) (2.1.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.5)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.4.9)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (3.5)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (2025.6.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (4.58.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug) (2.9.0.post0)\n",
            "Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imgaug\n",
            "Successfully installed imgaug-0.4.0\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install tensorflow\n",
        "!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite-runtime\n",
        "!pip install efficientnet\n",
        "!pip install imgaug albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0PAvx4IX8roM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from efficientnet.tfkeras import EfficientNetB0\n",
        "import logging\n",
        "import datetime\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import logging\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ispiWEQn8roN",
        "outputId": "7cd99750-d017-4420-c2a3-4fbc2b509b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "\n",
            "CUDA version:\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "\n",
            "\n",
            "NVIDIA-SMI:\n",
            "Tue Jun 17 20:27:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0             57W /  400W |     423MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def check_cuda():\n",
        "    print(\"Python version:\", sys.version)\n",
        "    print(\"\\nCUDA version:\")\n",
        "    try:\n",
        "        nvcc_output = subprocess.check_output(\"nvcc --version\", shell=True)\n",
        "        print(nvcc_output.decode())\n",
        "    except:\n",
        "        print(\"CUDA not found or nvcc not in PATH\")\n",
        "\n",
        "    print(\"\\nNVIDIA-SMI:\")\n",
        "    try:\n",
        "        nvidia_smi_output = subprocess.check_output(\"nvidia-smi\", shell=True)\n",
        "        print(nvidia_smi_output.decode())\n",
        "    except:\n",
        "        print(\"nvidia-smi not found\")\n",
        "\n",
        "check_cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "KJiA7nlj8roN",
        "outputId": "c64e8baa-9d16-4b5f-ad23-fbc6e4587419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.10.2.21\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cudnn-cu12==9.10.2.21) (12.5.3.2)\n",
            "Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cudnn-cu12\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.10.2.21 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.10.2.21\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "6031d6aa216c41d5a84a0655abd57c95"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install the latest compatible cuDNN version\n",
        "!pip install nvidia-cudnn-cu12==9.10.2.21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS9xTgyy8roN",
        "outputId": "9b0d0a38-8fac-40a5-bd49-a5aa3d001919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "GPU Device Name:  /device:GPU:0\n",
            "GPU memory growth enabled\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "\n",
        "# Print TensorFlow information\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "print(\"GPU Device Name: \", tf.test.gpu_device_name())\n",
        "\n",
        "# Configure GPU memory growth\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "        print(\"GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error setting GPU memory growth: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7e5itq638roO"
      },
      "outputs": [],
      "source": [
        "class_names = [\n",
        "    'ascaris_lumbricoides',\n",
        "    'trichuris_trichiura',\n",
        "    'hookworms',\n",
        "    'enterobius_vermicularis',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "enE5AeGx8roO"
      },
      "outputs": [],
      "source": [
        "def setup_logging():\n",
        "    log_dir = 'logs'\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    log_file = f'{log_dir}/training_{timestamp}.log'\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "    return log_file\n",
        "\n",
        "def preprocess_image(img):\n",
        "    try:\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        img = tf.image.per_image_standardization(img)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in preprocessing: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def save_model_version(model, version_info):\n",
        "    version_dir = 'model_versions'\n",
        "    if not os.path.exists(version_dir):\n",
        "        os.makedirs(version_dir)\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    version_path = f'{version_dir}/model_v{timestamp}'\n",
        "    os.makedirs(version_path)\n",
        "\n",
        "    model.save(f'{version_path}/model.h5')\n",
        "    with open(f'{version_path}/version_info.json', 'w') as f:\n",
        "        json.dump(version_info, f)\n",
        "\n",
        "    logging.info(f\"Model saved to {version_path}\")\n",
        "    return version_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eMp0Cgd97bB",
        "outputId": "7a7f7c9e-feba-4be3-9ac2-5871af457937"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sIBP_J2D8roO"
      },
      "outputs": [],
      "source": [
        "class LoggingCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logging.info(f\"Epoch {epoch+1} - loss: {logs['loss']:.4f} - accuracy: {logs['accuracy']:.4f} - val_loss: {logs['val_loss']:.4f} - val_accuracy: {logs['val_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6kZYE92y8roO"
      },
      "outputs": [],
      "source": [
        "def validate_data_directories():\n",
        "    train_dir = '/content/drive/MyDrive/MAPUA/dataset/train'\n",
        "    val_dir = '/content/drive/MyDrive/MAPUA/dataset/validate'\n",
        "\n",
        "    if not os.path.exists(train_dir) or not os.path.exists(val_dir):\n",
        "        raise ValueError(\"Training or validation directory not found\")\n",
        "\n",
        "    for class_name in class_names:\n",
        "        train_class_dir = os.path.join(train_dir, class_name)\n",
        "        val_class_dir = os.path.join(val_dir, class_name)\n",
        "\n",
        "        train_count = len(os.listdir(train_class_dir))\n",
        "        val_count = len(os.listdir(val_class_dir))\n",
        "\n",
        "        logging.info(f\"{class_name}: {train_count} training images, {val_count} validation images\")\n",
        "\n",
        "        if train_count < 10:\n",
        "            logging.warning(f\"Low number of training images for {class_name}: {train_count}\")\n",
        "        if val_count < 5:\n",
        "            logging.warning(f\"Low number of validation images for {class_name}: {val_count}\")\n",
        "\n",
        "# Run validation\n",
        "try:\n",
        "    validate_data_directories()\n",
        "    logging.info(\"Data validation completed successfully\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Data validation failed: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFrB4rsn8roO",
        "outputId": "69dde90e-6dcd-4ef7-8813-9344eabb2050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4000 images belonging to 4 classes.\n",
            "Found 800 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/MAPUA/dataset/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/MAPUA/dataset/validate',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xrxbLxLI8roO"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, validation_generator, class_names):\n",
        "    try:\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in range(len(validation_generator)):\n",
        "            x, y = validation_generator[i]\n",
        "            preds = model.predict(x)\n",
        "            y_pred.extend(np.argmax(preds, axis=1))\n",
        "            y_true.extend(np.argmax(y, axis=1))\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        plt.figure(figsize=(10,8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names,\n",
        "                   yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.yticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('confusion_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "        report = classification_report(y_true, y_pred,\n",
        "                                    target_names=class_names,\n",
        "                                    output_dict=True)\n",
        "        logging.info(\"Classification Report:\")\n",
        "        logging.info(json.dumps(report, indent=4))\n",
        "\n",
        "        system_accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
        "        logging.info(f\"System Accuracy: {system_accuracy:.4f}\")\n",
        "\n",
        "        return report, system_accuracy\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in evaluation: {str(e)}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leE4QFiY8roO",
        "outputId": "5f5449f4-beb1-4d7e-b99d-9bf91ee2a304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.5950 - loss: 1.0169 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1111s\u001b[0m 17s/step - accuracy: 0.5976 - loss: 1.0118 - val_accuracy: 0.6393 - val_loss: 0.8801 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m 1/62\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 68ms/step - accuracy: 0.9219 - loss: 0.2759"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 761ms/step - accuracy: 0.9219 - loss: 0.2759 - val_accuracy: 0.6354 - val_loss: 0.8825 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9440 - loss: 0.1874"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 3s/step - accuracy: 0.9442 - loss: 0.1867 - val_accuracy: 0.7526 - val_loss: 0.7384 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m 1/62\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0360"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 613ms/step - accuracy: 1.0000 - loss: 0.0360 - val_accuracy: 0.7539 - val_loss: 0.7429 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9776 - loss: 0.0899"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 3s/step - accuracy: 0.9776 - loss: 0.0899 - val_accuracy: 0.8008 - val_loss: 0.6184 - learning_rate: 1.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m 1/62\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.9688 - loss: 0.0459"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 591ms/step - accuracy: 0.9688 - loss: 0.0459 - val_accuracy: 0.7982 - val_loss: 0.6058 - learning_rate: 1.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9839 - loss: 0.0545"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 3s/step - accuracy: 0.9839 - loss: 0.0545 - val_accuracy: 0.8229 - val_loss: 0.5725 - learning_rate: 1.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m 1/62\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.9844 - loss: 0.0539"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 593ms/step - accuracy: 0.9844 - loss: 0.0539 - val_accuracy: 0.8294 - val_loss: 0.5346 - learning_rate: 1.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9912 - loss: 0.0305"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 3s/step - accuracy: 0.9912 - loss: 0.0306 - val_accuracy: 0.8581 - val_loss: 0.4366 - learning_rate: 1.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m 1/62\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.9844 - loss: 0.0372"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 604ms/step - accuracy: 0.9844 - loss: 0.0372 - val_accuracy: 0.8542 - val_loss: 0.4337 - learning_rate: 1.0000e-04\n"
          ]
        }
      ],
      "source": [
        "# Initialize logging\n",
        "log_file = setup_logging()\n",
        "logging.info(\"Starting training process\")\n",
        "\n",
        "# Define number of classes\n",
        "num_classes = 4  # Your actual number of classes\n",
        "\n",
        "# Create model\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "output = tf.keras.layers.Dense(num_classes, activation='softmax')(x)  # Changed from 5 to num_classes\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-6\n",
        "        ),\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'best_model.h5',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True\n",
        "        ),\n",
        "        LoggingCallback(),\n",
        "        tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPv3qwND8roP",
        "outputId": "1bf57504-3ec0-4f65-e037-6d1258b1634b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n"
          ]
        }
      ],
      "source": [
        "# Save model version\n",
        "version_info = {\n",
        "    'model_type': 'EfficientNetB0',\n",
        "    'input_shape': (224, 224, 3),\n",
        "    'num_classes': 4,\n",
        "    'training_date': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'final_accuracy': history.history['accuracy'][-1],\n",
        "    'final_val_accuracy': history.history['val_accuracy'][-1]\n",
        "}\n",
        "model_version_path = save_model_version(model, version_info)\n",
        "\n",
        "# Run evaluation\n",
        "report, system_accuracy = evaluate_model(model, validation_generator, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkpGns0w8roP",
        "outputId": "6ab06452-96d7-40d3-bbe1-001b31011f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpmagg87bd'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_932')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136695458951440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458944720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458954320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458946256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458947408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458946064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458953168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458945488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458953360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458955088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458939728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458942608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458946832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458942224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458950288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458939152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460609104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458949904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695458940496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460607952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460609680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460605648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460607184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460607376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460603152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460603728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460601232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460603536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460608912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460599888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460594512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460598544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460596624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460598352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460606608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460598736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460596432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460594128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460593744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460117776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460116432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460117008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460117584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460114320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460111248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460112400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460112976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460113744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460108944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460110096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460113360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460104144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460115856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460105872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460102608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460107600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460107792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460106640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459638032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459641680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460102992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460109328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459639184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459639760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459637648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459639568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459641872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459636304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459635536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459636496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459634384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459636880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459632848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459630160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459629776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459633808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459627280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459627088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459627856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459630928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459643216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461262736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461264080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461260816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461263504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461264656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461258512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461259664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461262160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461253712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461264464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461255440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461252176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461257168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461257360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461256208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461253328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461250640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461251984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461253520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695461250064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460837520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460838096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460837904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460827920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460835024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460834256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460836176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460833104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460839056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460831568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460828880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460828496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460832528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460825424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460825232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460824080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460823312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460829072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460836944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462499664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462510416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460830224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695460827536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462506384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462507536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462506960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462501584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462508880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462503312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462498128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462505040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462505232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462504080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462500240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462497936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462499472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462501392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462494480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136696606916048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462500816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462497744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136696606916432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695462494288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534181648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534181072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534180688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534169552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534178576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534176656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534179536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534178768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534173584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534171664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534171472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534176272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534174352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534169744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534173776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533758928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534173392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697534175696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533758352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533757584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533756432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533758736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533752208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533748944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533750288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533752016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533748560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533751056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533752400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533746640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533744720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533744336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533743952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533757776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533745872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697533753552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697930071120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697930069200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697930070736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697930064208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697930064976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697930066704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697930068240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697930067856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459387280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459386512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136697930061712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459389776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459385168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459384784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459383440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459381328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459387856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459382864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459381904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459384016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459384208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459397072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459384400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459385744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459386320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459385552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459386704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459389008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459389200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459389392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459386128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459391888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459392848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459389968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459390736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459388432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459393040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459391312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459391120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459389584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136695459393808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378875664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378886992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378886416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378875856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378887952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378887568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378888912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378888144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378885840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378889872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378889488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378890832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378890064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378886032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378886800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378888336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693378891408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375239376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375238608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375238224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375240720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375239568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375239760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375240144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375242064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375243024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375242256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375240912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375243984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375243600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375244944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375244176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375241872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375245904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375246096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375244368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375246480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375242448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375247632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375248400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375247248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375247056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375247824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375249744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375250704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375249936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375248592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375251664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375251280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375252624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375251856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375249552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375253584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375253200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375254160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375254352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375253776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375715280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375716432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375714896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375716240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375717584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375714320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375716816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375714512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375713552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375718160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375717200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375719120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375718352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375714128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375720080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375720272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375718544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375720656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375714704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375721808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375722576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375721424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375721232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375723536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375723152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375724496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375723728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375720464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375723920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136693375726416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFLite model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Define representative dataset generator\n",
        "def representative_dataset_gen():\n",
        "    num_calibration_samples = 100\n",
        "    for i in range(min(num_calibration_samples, len(validation_generator))):\n",
        "        image_batch, _ = validation_generator[i]\n",
        "        yield [image_batch]\n",
        "\n",
        "# Convert to TFLite with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "converter.full_integer_quantization = True\n",
        "\n",
        "# Convert the model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save TFLite model\n",
        "with open('parasite_classifier.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"TFLite model saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTh-5AR1WLdc"
      },
      "source": [
        "# Code for the raspbery pi app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyigCXUYWYLx"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# from tflite_runtime.interpreter import Interpreter\n",
        "# import logging\n",
        "# import os\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Setup logging for Raspberry Pi\n",
        "# def setup_logging():\n",
        "#     log_dir = 'logs'\n",
        "#     if not os.path.exists(log_dir):\n",
        "#         os.makedirs(log_dir)\n",
        "\n",
        "#     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "#     log_file = f'{log_dir}/prediction_{timestamp}.log'\n",
        "\n",
        "#     logging.basicConfig(\n",
        "#         level=logging.INFO,\n",
        "#         format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "#         handlers=[\n",
        "#             logging.FileHandler(log_file),\n",
        "#             logging.StreamHandler()\n",
        "#         ]\n",
        "#     )\n",
        "#     return log_file\n",
        "\n",
        "# def preprocess_image(img):\n",
        "#     try:\n",
        "#         img = img.astype(np.float32) / 255.0\n",
        "#         return img\n",
        "#     except Exception as e:\n",
        "#         logging.error(f\"Error in preprocessing: {str(e)}\")\n",
        "#         return None\n",
        "\n",
        "# def classify_image(image_path, interpreter, input_details, output_details, class_names, threshold=0.7):\n",
        "#     try:\n",
        "#         # Read image\n",
        "#         img = cv2.imread(image_path)\n",
        "#         if img is None:\n",
        "#             logging.error(f\"Failed to read image: {image_path}\")\n",
        "#             return None, \"Failed to read image\"\n",
        "\n",
        "#         # Resize image\n",
        "#         img = cv2.resize(img, (224, 224))\n",
        "\n",
        "#         # Preprocess image\n",
        "#         img = preprocess_image(img)\n",
        "#         if img is None:\n",
        "#             return None, \"Error in preprocessing\"\n",
        "\n",
        "#         # Prepare input tensor\n",
        "#         input_data = np.expand_dims(img, axis=0)\n",
        "\n",
        "#         # Set input tensor\n",
        "#         interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "#         # Run inference\n",
        "#         interpreter.invoke()\n",
        "\n",
        "#         # Get output tensor\n",
        "#         output_data = interpreter.get_tensor(output_details[0]['index'])[0]\n",
        "\n",
        "#         # Get prediction\n",
        "#         max_prob = np.max(output_data)\n",
        "#         if max_prob >= threshold:\n",
        "#             class_id = np.argmax(output_data)\n",
        "#             return class_names[class_id], max_prob\n",
        "#         else:\n",
        "#             return \"No confident prediction\", max_prob\n",
        "\n",
        "#     except Exception as e:\n",
        "#         logging.error(f\"Error in classification: {str(e)}\")\n",
        "#         return None, \"Error in classification\"\n",
        "\n",
        "# def main():\n",
        "#     # Setup logging\n",
        "#     log_file = setup_logging()\n",
        "#     logging.info(\"Starting parasite classification\")\n",
        "\n",
        "#     # Class names\n",
        "#     class_names = [\n",
        "#         'ascaris_lumbricoides',\n",
        "#         'trichuris_trichiura',\n",
        "#         'hookworms',\n",
        "#         'enterobius_vermicularis',\n",
        "#         'giardia_lamblia'\n",
        "#     ]\n",
        "\n",
        "#     try:\n",
        "#         # Load TFLite model\n",
        "#         interpreter = Interpreter(model_path='parasite_classifier.tflite')\n",
        "#         interpreter.allocate_tensors()\n",
        "\n",
        "#         # Get I/O details\n",
        "#         input_details = interpreter.get_input_details()\n",
        "#         output_details = interpreter.get_output_details()\n",
        "\n",
        "#         # Create directory for captured images if it doesn't exist\n",
        "#         images_dir = 'captured_images'\n",
        "#         if not os.path.exists(images_dir):\n",
        "#             os.makedirs(images_dir)\n",
        "\n",
        "#         while True:\n",
        "#             # Wait for user input\n",
        "#             input(\"Press Enter to capture and classify an image (or Ctrl+C to exit)...\")\n",
        "\n",
        "#             # Generate timestamp for image filename\n",
        "#             timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "#             image_path = f'{images_dir}/parasite_{timestamp}.jpg'\n",
        "\n",
        "#             # Capture image using Raspberry Pi camera\n",
        "#             # Note: You'll need to implement the actual camera capture code here\n",
        "#             # This is a placeholder for the camera capture\n",
        "#             # You might want to use picamera2 or other camera libraries\n",
        "#             # For example:\n",
        "#             # with picamera2.Picamera2() as camera:\n",
        "#             #     camera.capture_file(image_path)\n",
        "\n",
        "#             logging.info(f\"Image captured and saved to: {image_path}\")\n",
        "\n",
        "#             # Classify the image\n",
        "#             prediction, confidence = classify_image(\n",
        "#                 image_path,\n",
        "#                 interpreter,\n",
        "#                 input_details,\n",
        "#                 output_details,\n",
        "#                 class_names\n",
        "#             )\n",
        "\n",
        "#             if prediction:\n",
        "#                 result = f\"Prediction: {prediction} (Confidence: {confidence:.2f})\"\n",
        "#                 logging.info(result)\n",
        "#                 print(result)\n",
        "\n",
        "#                 # Display the image with prediction\n",
        "#                 img = cv2.imread(image_path)\n",
        "#                 cv2.putText(\n",
        "#                     img,\n",
        "#                     result,\n",
        "#                     (10, 30),\n",
        "#                     cv2.FONT_HERSHEY_SIMPLEX,\n",
        "#                     1,\n",
        "#                     (0, 255, 0),\n",
        "#                     2\n",
        "#                 )\n",
        "#                 cv2.imshow('Parasite Classification', img)\n",
        "#                 cv2.waitKey(0)\n",
        "#                 cv2.destroyAllWindows()\n",
        "#             else:\n",
        "#                 logging.error(\"Classification failed\")\n",
        "#                 print(\"Classification failed\")\n",
        "\n",
        "#     except KeyboardInterrupt:\n",
        "#         logging.info(\"Program terminated by user\")\n",
        "#         print(\"\\nProgram terminated by user\")\n",
        "#     except Exception as e:\n",
        "#         logging.error(f\"Unexpected error: {str(e)}\")\n",
        "#         print(f\"Unexpected error: {str(e)}\")\n",
        "#     finally:\n",
        "#         cv2.destroyAllWindows()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}